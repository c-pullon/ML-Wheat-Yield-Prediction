{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63753e0",
   "metadata": {},
   "source": [
    "# ML Wheat Yield Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ae20e",
   "metadata": {},
   "source": [
    "This code uses yearly data on temperature, rainfall, and wheat production in the top 5 wheat-producing states in Australia from 1901-2021. The data is merged and preprocessed for ML methods. FF-ANN, LSTM, and XGBoost models are constructed, trained, optimised and evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fd6fa",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b543ba8",
   "metadata": {},
   "source": [
    "#### Install libraries if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib seaborn scikit-learn scikeras keras tensorflow lightgbm xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2229290",
   "metadata": {},
   "source": [
    "#### Open csv files as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "aus_wheat = pd.read_csv('aus_wheat_yield.csv')\n",
    "aus_area_planted = pd.read_csv('hectares_planted.csv')\n",
    "mean_temp = pd.read_csv('aus_mean_temp.csv')\n",
    "min_temp = pd.read_csv('aus_min_temp.csv')\n",
    "max_temp = pd.read_csv('aus_max_temp.csv')\n",
    "precipitation = pd.read_csv('aus_precipitation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab8a55",
   "metadata": {},
   "source": [
    "#### Merge dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47232d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of regions\n",
    "regions = ['Australia', 'New South Wales', 'Victoria', 'Queensland', 'South Australia', 'Western Australia', 'Tasmania']\n",
    "\n",
    "# Create an empty list to store the individual DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over the regions and merge the data\n",
    "for region in regions:\n",
    "    region_yield_df = aus_wheat[['Year', region]].rename(columns={region: 'Wheat Yield'})\n",
    "    region_area_planted_df = aus_area_planted[['Year', region]].rename(columns={region: 'Hectares Planted'})\n",
    "    region_mean_temp_df = mean_temp[['Year', region]].rename(columns={region: 'Mean Temperature'})\n",
    "    region_max_temp_df = max_temp[['Year', region]].rename(columns={region: 'Max Temperature'})\n",
    "    region_min_temp_df = min_temp[['Year', region]].rename(columns={region: 'Min Temperature'})\n",
    "    region_precipitation_df = precipitation[['Year', region]].rename(columns={region: 'Precipitation'})\n",
    "\n",
    "    region_data = pd.merge(region_yield_df, region_mean_temp_df, on='Year')\n",
    "    region_data = pd.merge(region_data, region_max_temp_df, on='Year')\n",
    "    region_data = pd.merge(region_data, region_min_temp_df, on='Year')\n",
    "    region_data = pd.merge(region_data, region_precipitation_df, on='Year')\n",
    "    region_data = pd.merge(region_data, region_area_planted_df, on='Year')\n",
    "\n",
    "    # Add the 'Region' column\n",
    "    region_data['Region'] = region\n",
    "\n",
    "    dataframes.append(region_data)\n",
    "\n",
    "# Concatenate all the region DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by 'Year' to have the years in order\n",
    "merged_df.sort_values(by='Year', inplace=True)\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove unneeded regions\n",
    "df_filtered = merged_df[(merged_df['Region'] != 'Australia') & (merged_df['Region'] != 'Tasmania')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77815c46",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ecd22",
   "metadata": {},
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea0ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables for which you want summary statistics\n",
    "variables_of_interest = ['Wheat Yield', 'Mean Temperature', 'Min Temperature', 'Max Temperature', 'Precipitation', 'Hectares Planted']\n",
    "\n",
    "# Group the data by 'Region' and calculate specific summary statistics for each variable\n",
    "summary_stats = df_filtered.groupby('Region')[variables_of_interest].agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "# Display the summary statistics table for each variable\n",
    "print(\"Summary Statistics for Variables by Region:\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da6a24",
   "metadata": {},
   "source": [
    "#### Plot wheat yield over time for all five states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc013ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of regions\n",
    "regions = ['New South Wales', 'Victoria', 'Queensland', 'South Australia', 'Western Australia']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Iterate over the regions and plot wheat yield for each one\n",
    "for region in regions:\n",
    "    region_data = df_filtered[df_filtered['Region'] == region]\n",
    "    plt.plot(region_data['Year'], region_data['Wheat Yield'], label=region)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Wheat Yield')\n",
    "plt.title('Wheat Yield by Region over Time')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a98c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a 3x2 grid of subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over the regions and create plots in the grid\n",
    "for i, region in enumerate(regions):\n",
    "    # Filter data for the current region\n",
    "    region_data = df_filtered[df_filtered['Region'] == region]\n",
    "\n",
    "    # Plot wheat yield for the current region\n",
    "    axes[i].plot(region_data['Year'], region_data['Wheat Yield'])\n",
    "    axes[i].set_xlabel('Year')\n",
    "    axes[i].set_ylabel('Wheat Yield')\n",
    "    axes[i].set_title(f'Wheat Yield in {region}')\n",
    "\n",
    "# Remove any empty subplots in the grid\n",
    "for i in range(len(regions), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90607743",
   "metadata": {},
   "source": [
    "#### Correlation heatmap for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbcad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Create a 3x2 grid of subplots\n",
    "num_rows = 3\n",
    "num_cols = 2\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(16, 12))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create correlation matrices and heatmaps for each state\n",
    "for i, region in enumerate(regions):\n",
    "    # Filter data for the current region\n",
    "    state_data = df_filtered[df_filtered['Region'] == region]\n",
    "\n",
    "    # Create a correlation matrix\n",
    "    correlation_matrix = state_data.corr()\n",
    "\n",
    "    # Create a heatmap in the current subplot\n",
    "    sns.heatmap(correlation_matrix, ax=axes[i], annot=True, cmap='coolwarm')\n",
    "    axes[i].set_title(f'Correlation Matrix for {region}')\n",
    "\n",
    "# Remove any empty subplots in the grid\n",
    "for i in range(len(regions), num_rows * num_cols):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292a0e0",
   "metadata": {},
   "source": [
    "#### Correlation heatmap for all states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48179185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap for the overall correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_filtered.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix for All Regions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63391e4",
   "metadata": {},
   "source": [
    "#### Perform one-hot encoding to create boolean columns for each state, ready for neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678fe438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding for the 'Region' column\n",
    "encoded_regions = pd.get_dummies(df_filtered['Region'], prefix='Region')\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "df_filtered = pd.concat([df_filtered, encoded_regions], axis=1)\n",
    "\n",
    "# Define the inputs\n",
    "inputs = ['Mean Temperature', 'Min Temperature', 'Max Temperature', 'Precipitation', 'Hectares Planted'] + list(encoded_regions.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7128e",
   "metadata": {},
   "source": [
    "#### Center and scale data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Compute the mean of each column in the data\n",
    "column_means = np.mean(df_filtered[inputs], axis=0)\n",
    "\n",
    "# Subtract the column means from each column in the data\n",
    "centered_data = df_filtered[inputs] - column_means\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the centered data\n",
    "scaled_data = scaler.fit_transform(centered_data)\n",
    "\n",
    "# Assign the scaled data back to the original dataframe\n",
    "df_filtered[inputs] = scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a91e06",
   "metadata": {},
   "source": [
    "#### Save pre-processed dataframe as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('prepared_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504eeac",
   "metadata": {},
   "source": [
    "#### Split data into training and test sets based on the fixed split year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6961ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered[inputs]\n",
    "y = df_filtered['Wheat Yield']\n",
    "\n",
    "# Fixed split is used as this is time series data, a random split would disrupt the temporal nature and introduce information\n",
    "#leakage\n",
    "split_year = 1990\n",
    "X_train = df_filtered[df_filtered['Year'] < split_year][inputs]\n",
    "y_train = df_filtered[df_filtered['Year'] < split_year]['Wheat Yield']\n",
    "\n",
    "X_test = df_filtered[df_filtered['Year'] >= split_year][inputs]\n",
    "y_test = df_filtered[df_filtered['Year'] >= split_year]['Wheat Yield']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b106942",
   "metadata": {},
   "source": [
    "## Feed-forward ANN construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbcfdd9",
   "metadata": {},
   "source": [
    "#### Train FF-ANN and evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a450d22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Function to calculate Mean Absolute Percentage Error (MAPE)\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true), K.epsilon(), None))\n",
    "    return 100.0 * K.mean(diff, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_single_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(8, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))  \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "ANN = create_single_model()\n",
    "\n",
    "# Compile the model\n",
    "ANN.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mape'])\n",
    "\n",
    "# Train the model using all data\n",
    "history = ANN.fit(X_train, y_train, epochs=100, batch_size=2, verbose=1)\n",
    "\n",
    "loss, mae, mape_value = ANN.evaluate(X_test, y_test)\n",
    "predictions = ANN.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "r2 = r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c156ce",
   "metadata": {},
   "source": [
    "#### FF-ANN Loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70098124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MSE values from the training history\n",
    "train_loss = history.history['loss']\n",
    "\n",
    "# Plot the MSE during training\n",
    "plt.plot(train_loss)\n",
    "plt.title('Training MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2d1bd",
   "metadata": {},
   "source": [
    "#### FF-ANN evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print evaluation metrics\n",
    "print('MSE (FF-ANN) = ', loss)\n",
    "print('MAE (FF-ANN) = ', mae)\n",
    "print('RMSE (FF-ANN) = ', rmse)\n",
    "print('MAPE (FF-ANN) = ', mape_value)\n",
    "print('R2 (FF-ANN) = ', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9e180",
   "metadata": {},
   "source": [
    "#### FF-ANN state metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the regions and evaluate the single model for each region\n",
    "for region in regions:\n",
    "    region_data = df_filtered[df_filtered['Region'] == region]\n",
    "    X_test_region = region_data[region_data['Year'] >= split_year][inputs]\n",
    "    y_test_region = region_data[region_data['Year'] >= split_year]['Wheat Yield']\n",
    "\n",
    "    # Make predictions for the region using the single model\n",
    "    predictions_region = ANN.predict(X_test_region)\n",
    "\n",
    "    # Calculate evaluation metrics for the region\n",
    "    mse_region = mean_squared_error(y_test_region, predictions_region)\n",
    "    mae_region = mean_absolute_error(y_test_region, predictions_region)\n",
    "    rmse_region = np.sqrt(mse_region)\n",
    "    r2_region = r2_score(y_test_region, predictions_region)\n",
    "    mape_region = mape(y_test_region, predictions_region)\n",
    "\n",
    "    # Display the evaluation results for the region\n",
    "    print(f\"\\nResults for {region}:\")\n",
    "    print(\"MSE:\", mse_region)\n",
    "    print(\"MAE:\", mae_region)\n",
    "    print(\"RMSE:\", rmse_region)\n",
    "    print(\"R2 Score:\", r2_region)\n",
    "    print(\"MAPE:\", np.mean(mape_region))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b37d4",
   "metadata": {},
   "source": [
    "#### FF-ANN predicted vs actual test set values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16130a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x2 grid of subplots for the predicted vs. actual yield plots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n",
    "fig.suptitle('Predicted vs. Actual Wheat Yield for Each State (Feedforward ANN)', fontsize=16)\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over the regions and create plots in the grid\n",
    "for i, region in enumerate(regions):\n",
    "    region_data = df_filtered[df_filtered['Region'] == region]\n",
    "    X_test_region = region_data[region_data['Year'] >= split_year][inputs]\n",
    "    y_test_region = region_data[region_data['Year'] >= split_year]['Wheat Yield']\n",
    "    predictions_region = ANN.predict(X_test_region)\n",
    "\n",
    "    # Get the years in the test set for this region\n",
    "    years_test_region = region_data[region_data['Year'] >= split_year]['Year']\n",
    "\n",
    "    # Plot the actual yield as a blue line\n",
    "    axes[i].plot(years_test_region, y_test_region, label='Actual Yield', color='blue')\n",
    "    \n",
    "    # Plot the predicted yield as an orange line\n",
    "    axes[i].plot(years_test_region, predictions_region, label='Predicted Yield', color='orange')\n",
    "    \n",
    "    axes[i].set_xlabel('Year')\n",
    "    axes[i].set_ylabel('Yield')\n",
    "    axes[i].set_title(region)\n",
    "    axes[i].legend()\n",
    "\n",
    "# Remove any empty subplots in the grid\n",
    "for i in range(len(regions), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b6110",
   "metadata": {},
   "source": [
    "## LSTM construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58661784",
   "metadata": {},
   "source": [
    "#### LSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cddb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Define generator\n",
    "n_input = 4\n",
    "n_features = X_train.shape[1] \n",
    "\n",
    "generator = TimeseriesGenerator(X_train.values, y_train.values, length=n_input, batch_size=4)\n",
    "\n",
    "# Continue with model definition and training\n",
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(LSTM(32, kernel_regularizer=l2(0.05), return_sequences=True, input_shape=(n_input, n_features)))\n",
    "LSTM_model.add(Dropout(0.1))\n",
    "LSTM_model.add(LSTM(8, kernel_regularizer=l2(0.05)))\n",
    "LSTM_model.add(Dropout(0.1))\n",
    "LSTM_model.add(Dense(1))\n",
    "LSTM_model.compile(loss='mean_squared_error', optimizer= 'adam')\n",
    "LSTM_model.summary()\n",
    "\n",
    "\n",
    "LSTM_model.fit(generator, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f5fc9",
   "metadata": {},
   "source": [
    "#### LSTM Loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = LSTM_model.history.history['loss']\n",
    "plt.plot(range(len(loss_per_epoch)), loss_per_epoch)\n",
    "plt.title('Training MSE (LSTM)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ca59c",
   "metadata": {},
   "source": [
    "#### LSTM evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ec7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation on the test data\n",
    "test_generator = TimeseriesGenerator(X_test.values, y_test.values, length=n_input, batch_size=1)\n",
    "y_pred_test = LSTM_model.predict(test_generator)\n",
    "\n",
    "# Get the corresponding years for the test set\n",
    "years_test_lstm = df_filtered[df_filtered['Year'] >= split_year]['Year'][n_input:].values\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mse = mean_squared_error(y_test.values[n_input:], y_pred_test)\n",
    "test_mae = mean_absolute_error(y_test.values[n_input:], y_pred_test)\n",
    "r2 = r2_score(y_test.values[n_input:], y_pred_test)\n",
    "\n",
    "# Calculate RMSE and MAPE using existing functions\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mape = mape(y_test.values[n_input:], y_pred_test)\n",
    "\n",
    "print(\"LSTM MSE:\", test_mse)\n",
    "print(\"LSTM MAE:\", test_mae)\n",
    "print(\"LSTM R-squared:\", r2)\n",
    "print(\"LSTM RMSE:\", test_rmse)\n",
    "print(\"LSTM MAPE:\", np.mean(test_mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab063ee",
   "metadata": {},
   "source": [
    "#### LSTM predicted vs actual test set values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08075f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a 3x2 grid of subplots for the predicted vs. actual yield plots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n",
    "fig.suptitle('Predicted vs. Actual Wheat Yield for Each State (LSTM)', fontsize=16)\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over the regions and create plots in the grid\n",
    "for i, region in enumerate(regions):\n",
    "    region_data = df_filtered[df_filtered['Region'] == region]\n",
    "    X_test_region = region_data[region_data['Year'] >= split_year][inputs]\n",
    "    y_test_region = region_data[region_data['Year'] >= split_year]['Wheat Yield']\n",
    "    \n",
    "    # Generate sequences for LSTM predictions\n",
    "    lstm_test_generator = TimeseriesGenerator(X_test_region.values, y_test_region.values, length=n_input, batch_size=1)\n",
    "    lstm_predictions_region = LSTM_model.predict(lstm_test_generator)\n",
    "    \n",
    "    # Get the years in the test set for this region\n",
    "    years_test_region = region_data[region_data['Year'] >= split_year]['Year']\n",
    "    \n",
    "\n",
    "    # Plot the actual yield as a blue line\n",
    "    axes[i].plot(years_test_region, y_test_region, label='Actual Yield', color='blue')\n",
    "    \n",
    "    # Plot the predicted yield as an orange line (LSTM)\n",
    "    axes[i].plot(years_test_region.values[n_input:], lstm_predictions_region, label='LSTM Predicted Yield', color='orange')\n",
    "\n",
    "    \n",
    "    axes[i].set_xlabel('Year')\n",
    "    axes[i].set_ylabel('Yield')\n",
    "    axes[i].set_title(region)\n",
    "    axes[i].legend()\n",
    "\n",
    "# Remove any empty subplots in the grid\n",
    "for i in range(len(regions), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5457f",
   "metadata": {},
   "source": [
    "#### LSTM state metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77fff7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store state-wise results\n",
    "state_results_lstm = {}\n",
    "\n",
    "# Iterate over the regions and evaluate the LSTM model for each region\n",
    "for region in regions:\n",
    "    state_data = df_filtered[df_filtered['Region_' + region] == 1]\n",
    "    X_state = state_data[inputs]\n",
    "    y_state = state_data['Wheat Yield']\n",
    "\n",
    "    # Create a separate generator for predictions on the test set\n",
    "    state_test_generator_lstm = TimeseriesGenerator(X_state.values, y_state.values, length=n_input, batch_size=1)\n",
    "    y_pred_state_lstm = LSTM_model.predict(state_test_generator_lstm)\n",
    "\n",
    "    # Evaluate the model for the region on the test set\n",
    "    state_test_mse_lstm = mean_squared_error(y_state.values[n_input:], y_pred_state_lstm)\n",
    "    state_test_mae_lstm = mean_absolute_error(y_state.values[n_input:], y_pred_state_lstm)\n",
    "    state_r2_lstm = r2_score(y_state.values[n_input:], y_pred_state_lstm)\n",
    "    \n",
    "    # Calculate RMSE and MAPE using existing functions\n",
    "    state_test_rmse_lstm = np.sqrt(state_test_mse_lstm)\n",
    "    state_test_mape_lstm = mape(y_state.values[n_input:], y_pred_state_lstm)\n",
    "\n",
    "    # Store the results for the region\n",
    "    state_results_lstm[region] = {\n",
    "        'Test MSE': state_test_mse_lstm,\n",
    "        'Test MAE': state_test_mae_lstm,\n",
    "        'Test RMSE': state_test_rmse_lstm,\n",
    "        'Test MAPE': np.mean(state_test_mape_lstm),\n",
    "        'R-squared': state_r2_lstm\n",
    "    }\n",
    "\n",
    "print(\"State-wise Model Performance - LSTM:\")\n",
    "for region, results_lstm in state_results_lstm.items():\n",
    "    print(f\"Region: {region}\")\n",
    "    print(\"LSTM MSE:\", results_lstm['Test MSE'])\n",
    "    print(\"LSTM MAE:\", results_lstm['Test MAE'])\n",
    "    print(\"LSTM RMSE:\", results_lstm['Test RMSE'])\n",
    "    print(\"LSTM MAPE:\", results_lstm['Test MAPE'])\n",
    "    print(\"LSTM R-squared:\", results_lstm['R-squared'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f425ad",
   "metadata": {},
   "source": [
    "## XGBoost construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96acc6f",
   "metadata": {},
   "source": [
    "#### Grid search for optimal hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameters for the XGBoost model\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror', \n",
    "    'eval_metric': 'rmse',            \n",
    "}\n",
    "\n",
    "# Define the dataset\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.3, 0.6],       \n",
    "    'max_depth': [4, 6],                 \n",
    "    'min_child_weight': [1, 3],          \n",
    "    'subsample': [0.8, 1.0],                \n",
    "    'n_estimators': [50, 100],\n",
    "    'gamma': [0, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [0, 0.1, 1],\n",
    "}\n",
    "\n",
    "# Create an XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params_xgb = grid_search.best_params_\n",
    "print(\"Best Parameters for XGBoost:\", best_params_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f0ed3",
   "metadata": {},
   "source": [
    "#### XGBoost training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model with the best parameters\n",
    "best_xgb_model = xgb.XGBRegressor(**xgb_params, **best_params_xgb)\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test_xgb = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Get the corresponding years for the test set\n",
    "years_test_xgb = df_filtered[df_filtered['Year'] >= split_year]['Year'].values\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mse_xgb = mean_squared_error(y_test, y_pred_test_xgb)\n",
    "test_mae_xgb = mean_absolute_error(y_test, y_pred_test_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_test_xgb)\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_rmse_xgb = np.sqrt(test_mse_xgb)\n",
    "test_mape_xgb = np.mean(mape(y_test, y_pred_test_xgb))\n",
    "\n",
    "print(\"Test MSE (XGBoost):\", test_mse_xgb)\n",
    "print(\"Test MAE (XGBoost):\", test_mae_xgb)\n",
    "print(\"Test R-squared (XGBoost):\", r2_xgb)\n",
    "print(\"Test RMSE (XGBoost):\", test_rmse_xgb)\n",
    "print(\"Test MAPE (XGBoost):\", test_mape_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2f488",
   "metadata": {},
   "source": [
    "#### XGBoost predicted vs actual test set values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the results for XGBoost\n",
    "results_df_xgb = pd.DataFrame({\n",
    "    'Year': years_test_xgb,\n",
    "    'Actual_Yield': y_test.values,\n",
    "    'Predicted_Yield': y_pred_test_xgb\n",
    "})\n",
    "\n",
    "# Create a 3x2 grid of subplots for the predicted vs. actual yield plots for XGBoost\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n",
    "fig.suptitle('Predicted vs. Actual Wheat Yield for Each State (XGBoost)', fontsize=16)\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over the regions and create plots in the grid for XGBoost\n",
    "for i, region in enumerate(regions):\n",
    "    region_data = df_filtered[df_filtered['Region'] == region]\n",
    "    X_test_region = region_data[region_data['Year'] >= split_year][inputs]\n",
    "    y_test_region = region_data[region_data['Year'] >= split_year]['Wheat Yield']\n",
    "    \n",
    "    # Predictions for XGBoost\n",
    "    y_pred_region_xgb = best_xgb_model.predict(X_test_region)\n",
    "    \n",
    "    # Get the years in the test set for this region\n",
    "    years_test_region = region_data[region_data['Year'] >= split_year]['Year']\n",
    "    \n",
    "    # Plot the actual yield as a blue line\n",
    "    axes[i].plot(years_test_region, y_test_region, label='Actual Yield', color='blue')\n",
    "    \n",
    "    # Plot the predicted yield as a red line (XGBoost)\n",
    "    axes[i].plot(years_test_region.values, y_pred_region_xgb, label='XGBoost Predicted Yield', color='orange')\n",
    "\n",
    "    axes[i].set_xlabel('Year')\n",
    "    axes[i].set_ylabel('Yield')\n",
    "    axes[i].set_title(region)\n",
    "    axes[i].legend()\n",
    "\n",
    "# Remove any empty subplots in the grid for XGBoost\n",
    "for i in range(len(regions), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout and display the plots for XGBoost\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f37d94",
   "metadata": {},
   "source": [
    "#### XGBoost state metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b93714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store state-wise results for XGBoost\n",
    "state_results_xgb = {}\n",
    "\n",
    "# Iterate over the regions and evaluate the XGBoost model for each region\n",
    "for region in regions:\n",
    "    state_data = df_filtered[df_filtered['Region_' + region] == 1]\n",
    "    X_state = state_data[inputs]\n",
    "    y_state = state_data['Wheat Yield']\n",
    "\n",
    "    # Predictions for XGBoost\n",
    "    y_pred_state_xgb = best_xgb_model.predict(X_state)\n",
    "\n",
    "    # Evaluate the model for the region on the test set\n",
    "    state_test_mse_xgb = mean_squared_error(y_state, y_pred_state_xgb)\n",
    "    state_test_mae_xgb = mean_absolute_error(y_state, y_pred_state_xgb)\n",
    "    state_r2_xgb = r2_score(y_state, y_pred_state_xgb)\n",
    "    state_rmse_xgb = np.sqrt(state_test_mse_xgb)\n",
    "    state_mape_xgb = np.mean(mape(y_state, y_pred_state_xgb))\n",
    "\n",
    "    # Store the results for the region\n",
    "    state_results_xgb[region] = {\n",
    "        'Test MSE (XGBoost)': state_test_mse_xgb,\n",
    "        'Test MAE (XGBoost)': state_test_mae_xgb,\n",
    "        'R-squared (XGBoost)': state_r2_xgb,\n",
    "        'Test RMSE (XGBoost)': state_rmse_xgb,\n",
    "        'Test MAPE (XGBoost)': state_mape_xgb,\n",
    "    }\n",
    "\n",
    "# Output state-wise results for XGBoost\n",
    "print(\"State-wise Model Performance - XGBoost:\")\n",
    "for region, results_xgb in state_results_xgb.items():\n",
    "    print(f\"Region: {region}\")\n",
    "    print(\"Test MSE (XGBoost):\", results_xgb['Test MSE (XGBoost)'])\n",
    "    print(\"Test MAE (XGBoost):\", results_xgb['Test MAE (XGBoost)'])\n",
    "    print(\"R-squared (XGBoost):\", results_xgb['R-squared (XGBoost)'])\n",
    "    print(\"Test RMSE (XGBoost):\", results_xgb['Test RMSE (XGBoost)'])\n",
    "    print(\"Test MAPE (XGBoost):\", results_xgb['Test MAPE (XGBoost)'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a9d8ff",
   "metadata": {},
   "source": [
    "#### Show relative feature importance for environmental features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf67278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importance\n",
    "feature_importance = best_xgb_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame to store feature importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Define the environmental features you want to show\n",
    "selected_features = [\"Hectares Planted\", \"Precipitation\", \"Min Temperature\", \"Max Temperature\", \"Mean Temperature\"]\n",
    "\n",
    "# Filter feature importance DataFrame\n",
    "filtered_feature_importance_df = feature_importance_df[\n",
    "    feature_importance_df['Feature'].isin(selected_features)\n",
    "]\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "filtered_feature_importance_df = filtered_feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot filtered feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=filtered_feature_importance_df, palette='viridis')\n",
    "plt.title('XGBoost Feature Importance (Selected Environmental Features)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
